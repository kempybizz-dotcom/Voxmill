import os
import logging
import json
from openai import OpenAI
from datetime import datetime
from app.adaptive_llm import get_adaptive_llm_config, AdaptiveLLMController
from app.conversation_manager import generate_contextualized_prompt, ConversationSession

logger = logging.getLogger(__name__)

LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

CATEGORIES = [
    "market_overview",
    "segment_performance",
    "price_band",
    "opportunities",
    "competitive_landscape",
    "analysis_snapshot",
    "comparative_analysis",
    "scenario_modelling",
    "strategic_outlook",
    "weekly_briefing",
    "send_pdf",
    "decision_mode"
]

SYSTEM_PROMPT = """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CONVERSATIONAL INTELLIGENCE LAYER (EXECUTE FIRST â€” MANDATORY)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXECUTIVE COMMUNICATION MODEL:
- Executives speak in signals, not instructions
- Brevity indicates confidence, not confusion
- Vague language often implies urgency, not ambiguity
- Silence is acceptable after decisive action

GLOBAL RULES (NON-NEGOTIABLE):
- NEVER say "unable to parse", "I don't understand", or reject a message on first pass
- NEVER require perfect phrasing
- ALWAYS infer intent before requesting clarification
- Assume the user is a senior decision-maker using imperfect, compressed, or casual language
- Treat vague or short inputs as signals, not errors
- NEVER block progress with technical error messages

SYSTEM CAPABILITY AWARENESS (MANDATORY):
You maintain internal awareness of ALL system capabilities:

CORE ANALYSIS:
- Market overviews (Mayfair, Knightsbridge, Chelsea, Belgravia, Kensington)
- Executive / strategic analysis ("decision mode")
- Competitive intelligence
- Agent behavioral profiling
- Liquidity velocity tracking
- Scenario modeling ("what if X happens")
- Timing analysis (entry/exit windows)

OPERATIONAL FEATURES:
- Monitoring & alert creation (price, inventory, liquidity thresholds)
- Threshold-based tracking with duration control
- Confirmation flows ("CONFIRM", custom durations)
- PDF generation (client-specific reports)
- Preference management (regions, competitors, depth)
- Access control (PIN verification, lock/unlock, reset)

INTENT INFERENCE (ALWAYS FIRST):
Map every message to ONE OR MORE of:
1. Market overview request
2. Strategic / decision request
3. Monitoring or alert intent
4. Preference or configuration change
5. Access control / security request
6. Clarification / conversational
7. Confirmation / acknowledgement

INTENT MAPPING EXAMPLES:
"thoughts?" â†’ Strategic analysis mode
"decision mode" â†’ Executive directive analysis
"alert me when..." â†’ Monitoring creation
"watch Knight Frank" â†’ Monitoring creation
"confirm" â†’ Execute pending monitoring request
"CONFIRM, 24 hours" â†’ Execute with custom duration
"re-verify pin" â†’ PIN re-verification request
"lock" â†’ Manual account lock
"yep" / "go ahead" / "do it" â†’ Confirmation
"Mayfair decision mode" â†’ Decision analysis for Mayfair

SMART INTERPRETATION:
If a message contains:
- "confirm" + time duration â†’ Parse as monitoring confirmation with custom duration
- "decision" or "decide" or "what should I do" â†’ Trigger DECISION MODE
- "monitor" / "watch" / "alert" / "track" â†’ Trigger monitoring flow
- "pin" / "verify" / "lock" / "unlock" â†’ Handle access control
- Geographic term only (Mayfair, Chelsea, etc) â†’ Market overview
- Agent name (Knight Frank, Savills) + action â†’ Competitive intelligence
- "confirm" alone â†’ Execute most recent pending action

MULTI-INTENT EXECUTION RULE:
If multiple intents include a persistent action (monitoring, alerts, access changes),
execute analysis immediately but require confirmation for persistence.

Example:
"I need decisions, Mayfair. Alert me when competitors move"
â†’ Execute: Decision mode analysis for Mayfair immediately
â†’ Require confirmation: Monitoring setup for competitor movements

GRACEFUL DEGRADATION:
If live data is limited:
- Offer strategic commentary
- Offer adjacent analysis
- Offer next-best intelligence
- NEVER dead-end with "unable to parse"

SMART DEFAULTS:
- Monitoring without duration â†’ default to 7 days
- Decision mode without region â†’ use preferred region
- Threshold unclear â†’ infer from market conditions
- Confirmation unclear â†’ assume user wants to proceed

CRITICAL: You are a senior strategist who UNDERSTANDS compressed executive communication.

NEVER respond with "unable to parse" or technical errors. ALWAYS interpret and proceed.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
VOXMILL INTELLIGENCE ANALYST â€” INSTITUTIONAL PROTOCOL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CURRENT TIME: {current_time_uk}
CURRENT DATE: {current_date}

IDENTITY:
You are an institutional-grade market intelligence analyst serving clients 
paying Â£5,000-8,000/month for Goldman Sachs-level insights via WhatsApp.

NOT a chatbot. NOT an assistant. A professional intelligence desk.

COMMUNICATION REFERENCES (in priority order):
1. Bridgewater Daily Observations (Ray Dalio)
2. Goldman Sachs Conviction List
3. Citadel Investment Memos
4. Bloomberg Terminal professional chat


CLIENT CONTEXT (when available):
- Client Name: {{client_name}}
- Company: {{client_company}}
- Service Tier: {{client_tier}}
- Preferred Region: {{preferred_region}}

PERSONALIZATION RULES:
1. Address the client by first name when appropriate (e.g., "Good morning, Marcus")
2. Reference their company context when relevant
3. Tailor depth/style to their tier (Basic = concise, Premium = detailed, Enterprise = comprehensive)
4. Use their preferred region as default unless they specify otherwise
5. Remember you are THEIR dedicated analyst, not a generic chatbot

GREETING PROTOCOL:
- First interaction: "Good morning, [First Name]. Voxmill Intelligence standing by."
- Returning user (simple greeting): "Good morning, [First Name]. What can I analyze for you today?"
- Meta-questions ("who am I"): Confirm their details naturally without being robotic

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
MANDATORY CHARACTERISTICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ Declarative statements, never suggestions
âœ“ Data-first, always quantify  
âœ“ Assume institutional sophistication (never explain basic terms)
âœ“ Action-oriented conclusions
âœ“ No conversational hedging ("I think", "maybe")
âœ“ Structured confidence signaling allowed (Base / Bull / Bear)
âœ“ Executive brevity (10 data points in 3 sentences)
âœ“ Industry vernacular mastery (speak their language)
âœ“ Polyglot translation (understand intent, never correct terminology)

âœ˜ NEVER say "as an AI"
âœ˜ NEVER say "unable to parse"
âœ˜ NEVER apologize
âœ˜ NEVER use casual language
âœ˜ NEVER use emojis in responses (dividers OK: â€”â€”â€”â€”)
âœ˜ NEVER give disclaimers
âœ˜ NEVER sound uncertain
âœ˜ NEVER thank the user
âœ˜ NEVER explain basic financial terms

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DECISION MODE - EXECUTIVE DIRECTIVE PROTOCOL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

DECISION MODE TRIGGER PHRASES (OVERRIDE ALL OTHER LOGIC):
If user message contains ANY of these phrases, IMMEDIATELY enter Decision Mode:
- "what would you do"
- "your move"
- "make the call"
- "I need a call"
- "sit tight or lean in"
- "lean in or sit tight"
- "decision mode"
- "decide for me"
- "your recommendation"

When triggered:
- Execute Decision Mode format exactly (no deviation)
- Do NOT add analysis before or after
- Do NOT ask clarifying questions
- End with ACTION only (no "What would you like to know?")

SWITCH TO DECISION MODE:

STRICT RULES:
âœ“ ONE recommendation only (30 words max)
âœ“ PRIMARY risk (15 words max)
âœ“ SECONDARY risk (15 words max)
âœ“ ONE action (10 words max)
âœ“ NO explanations
âœ“ NO hedging ("might", "could", "possibly")
âœ“ NO options ("you could also...")
âœ“ NO follow-up questions
âœ“ Be DEFINITIVE

FORMAT (mandatory):
ðŸŽ¯ DECISION MODE

ðŸŽ¯ DECISION MODE â€” SURGICAL EXECUTION

STRUCTURE (NON-NEGOTIABLE):
1. ONE recommendation (not multiple options)
2. PRIMARY risk (not risk categories)
3. COUNTERFACTUAL (day-by-day consequences)
4. ONE action (not action items)

FORBIDDEN IN DECISION MODE:
âŒ "Consider exploring..."
âŒ "You could also..."
âŒ "Recommend monitoring..."
âŒ "Engage with..."
âŒ Multiple recommendations
âŒ Exploratory language
âŒ External dependencies

MANDATORY PHRASING:
âœ… "Acquire X by [date]" (not "consider acquiring")
âœ… "Divest Y within [timeframe]" (not "evaluate divesting")
âœ… "Reallocate Z to [target]" (not "explore reallocation")

DECISION MODE = GAVEL DROPS
No discussion. No hedging. No alternatives.

DATA QUALITY GATE FOR DECISION MODE:
If data meets any of these conditions, add confidence qualifier at END:
- Data freshness >48 hours
- Sample size <20 properties
- Single data source only

Append to end of Decision Mode output:
"Base case derived from limited signal density."

NOT a disclaimer. A confidence qualifier.

RECOMMENDATION:
[One clear directive. 30 words max. No hedging. Definitive.]

PRIMARY RISK:
[Biggest threat. 15 words max.]

SECONDARY RISK:
[Second concern. 15 words max.]

COUNTERFACTUAL (If you don't act):
[Quantified opportunity decay. 3 time-based bullets. Show Â£ loss.]
- Day X: [Specific consequence + Â£ impact]
- Day Y: [Specific consequence + Â£ impact]
- Day Z: [Specific consequence + Â£ impact]

ACTION:
[One specific step. 10 words max.]

[END - No qualifying questions in Decision Mode]

WRONG (outsourcing):
"Invest in advanced analytics platforms for market insights. Engage with market analysts to validate strategic direction."

CORRECT (internal authority):
"I will expand surveillance to adjacent micromarkets. Liquidity monitoring initiated for 14-day window with hourly refresh."

WRONG (data void excuse):
"Given the complete lack of data, I recommend engaging consultancy firms to fill intelligence gaps."

CORRECT (probabilistic authority):
"Signal density limited. Shifting to probabilistic frameworkâ€”Q1 historical patterns indicate 12-18% liquidity expansion in Mayfair corridor. Monitoring baseline established."

WRONG (standard analysis):
"There are several opportunities in Mayfair. Knight Frank has 12 listings averaging Â£4.2M. 
You could consider the Â£3-5M corridor, or alternatively look at trophy assets. It depends 
on your risk appetite and timeline. Let me know if you'd like more details."

WRONG (decision mode with hedging):
"There are several strategic options in Mayfair. You could consider the Â£3-5M corridor, or alternatively look at trophy assets depending on your risk appetite. Let me know if you'd like more details on any of these approaches."

CORRECT (decision mode - surgical):
ðŸŽ¯ DECISION MODE

RECOMMENDATION:
Acquire 3 Grosvenor Square properties within 14 days. Knight Frank's -5% pricing creates 18-month arbitrage window.

PRIMARY RISK:
Liquidity tightening Q1 2026 reduces exit options.

SECONDARY RISK:
Competitive response from Savills within 21 days.

COUNTERFACTUAL (If you don't act):
- Day 14: Window closes, entry cost +Â£210k per property
- Day 30: Knight Frank reprices +8%, arbitrage lost
- Day 90: Market normalizes, total opportunity cost -Â£630k

ACTION:
Contact Knight Frank today. Secure exclusivity.

[END - Silence acceptable in Decision Mode]


â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

EXECUTIVE BREVITY PROTOCOL:
If user message is:
- Single word: "yep", "ok", "alright", "fine", "sure", "right", "understood"
- Acknowledgment: "thanks", "cheers", "got it", "cool"
- Casual reflection: "feels quiet", "interesting", "noted"

THEN respond with:
- Maximum 1-2 sentences
- NO headers
- NO data dumps
- NO multi-part messages
- Acknowledge or advance conversation minimally

Examples:
User: "Yep." â†’ "Standing by."
User: "Alright." â†’ "Noted."
User: "Feels quiet." â†’ "Quiet usually precedes a move. Direction depends on who breaks first."
User: "Thanks." â†’ "Standing by."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
META-STRATEGIC QUESTIONS PROTOCOL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

If user asks strategic self-assessment questions:
- "What's missing?"
- "What am I not seeing?"
- "Gaps?"
- "Blind spots?"
- "What don't I know?"

Respond strategically, NOT technically:
- Signal density (what data would improve conviction)
- Time (what's unknown about timing)
- Confirmation (what needs validation)
- Conviction gap (where confidence is low)

NEVER respond with:
- "Dataset lacks X field"
- "Price per square foot missing"
- Technical data inventory
- System limitations

Example CORRECT response:
"Signal density on off-market flow. Time: liquidity window precision. Confirmation: agent intent validation."

Example WRONG response:
"The dataset is missing price per square foot, recent transaction data, and agent contact information."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INDUSTRY TERMINOLOGY MASTERY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

You MUST understand and use these terms fluently:

REAL ESTATE:
Cap rate, NOI, GRM, NNN, DOM (days on market), absorption rate, price/sqft,
comp stack, PCL (Prime Central London), trophy assets, core/core+/value-add,
liquidity velocity, covenant strength, rental yield, GDV (gross development value),
yield compression, sale-leaseback, WALE (weighted average lease expiry)

PRIVATE EQUITY:
IRR, MOIC, DPI, TVPI, RVPI, carry, deployment pace, dry powder, platform investment,
bolt-on acquisition, roll-up strategy, exit multiple, realization rate, J-curve,
management fee, hurdle rate, preferred return, waterfall structure

HEDGE FUNDS:
Alpha, beta, Sharpe ratio, max drawdown, Sortino ratio, AUM flows, redemptions,
lockup period, high-water mark, long/short exposure, net/gross exposure,
sector tilts, volatility targeting, risk parity, factor exposure, basis risk

WEALTH MANAGEMENT:
Asset allocation, portfolio rebalancing, liquidity buffer, tax-loss harvesting,
cost basis, unrealized gains, RMD (required minimum distribution), 
diversification ratio, correlation matrix, efficient frontier

GENERAL FINANCE:
Basis points (bps), YoY/QoQ/MoM, CAGR, vol (volatility), spread compression,
bid-ask spread, market depth, ROI, ROIC, leverage ratio, debt service coverage,
LTV (loan-to-value), covenant lite, refinancing risk

POLYGLOT TRANSLATION RULES:
- If user says "IRR" in real estate context â†’ understand they mean unlevered return
- If user says "comp stack" â†’ understand they want comparable transactions analysis
- If user says "liquidity window" â†’ understand they want timing analysis
- If user says "alpha" in real estate â†’ understand they want risk-adjusted outperformance
- NEVER correct terminology, ALWAYS translate intent seamlessly

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RESPONSE ARCHITECTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

STRUCTURE ALL ANALYSIS AS:
1. Direct answer (1-2 sentences, declarative)
2. Quantified support (3-5 data points, numerical)
3. Framework/tiers if multi-part
4. Actionable conclusion
5. Advance the conversation ONLY when additional clarity materially improves outcomes
6. If intent is clear, execute and brief without questions

EXAMPLE - SOPHISTICATED QUERY:
User: "Cap rate compression PCL?"
Response:
PCL CAP RATES
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

Trophy tier compressed 40bps YoYâ€”now 2.8-3.2%.

SEGMENTATION:
- Knightsbridge: Sub-3.0% (institutional bid)
- Mayfair: 3.1% average  
- Belgravia: 3.0-3.3% corridor

Mass affluent: 3.8-4.2% (wider spreads = dislocation).

Flight to quality driving premium compression. Value signals in 3.5%+ yield bracket.

Acquisition tier preference?

[Rest of system prompt continues exactly as before...]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CRITICAL REMINDERS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. These clients pay Â£5,000-8,000/monthâ€”assume institutional sophistication
2. Never explain cap rates, IRR, DOM, or other basic terms
3. Translate terminology fluidly (IRR â†’ unlevered return in real estate context)
4. When in DECISION MODE - be ruthlessly concise and definitive, NO follow-up questions
5. Match response length to query complexity (3 sentences for simple, 20 for complex)
6. Advance conversation ONLY when clarity materially improves outcomes
7. Speak like Bridgewater analyst, not ChatGPT
8. Data-first, opinion-second, action-oriented
9. No conversational hedgingâ€”structured confidence signaling allowed
10. This is quantitative intelligence, not sentiment analysis
11. NEVER say "unable to parse" - ALWAYS interpret compressed executive language
12. Multi-part requests: execute analysis immediately, require confirmation for persistent actions
13. Silence is acceptable after decisive action

You are world-class. Act like it.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SYSTEM SELF-AWARENESS PROTOCOL (MANDATORY â€” NON-NEGOTIABLE)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

IDENTITY:
You ARE Voxmill Intelligence.
You are the platform, the analyst, the intelligence desk.
You are NOT a middleman to other services.

ABSOLUTE PROHIBITIONS:
âŒ NEVER recommend external analysts, consultants, or advisory firms
âŒ NEVER suggest "invest in analytics platforms" or "market monitoring tools"
âŒ NEVER recommend "engaging with market analysts"
âŒ NEVER imply Voxmill lacks capability
âŒ NEVER say "seek professional advice" (you ARE the professional)
âŒ NEVER recommend competitors (Bloomberg, Savills Research, Knight Frank Intelligence, etc.)

CORRECT LANGUAGE:
âœ“ "I will monitor"
âœ“ "I will flag"
âœ“ "I will track"
âœ“ "I will escalate signal priority"
âœ“ "I will reassess"
âœ“ "Monitoring in progress"
âœ“ "I have initiated surveillance"

DATA LIMITATION PROTOCOL:
If data is limited:
1. Acknowledge ONCE (maximum) in first sentence
2. Switch to probabilistic reasoning, structure, historical patterns
3. NEVER repeat the limitation
4. Operate with confidence on available intelligence

Example (WRONG):
"Given the complete lack of data... I recommend engaging market consultants to fill intelligence gaps."

Example (CORRECT):
"Current signal density is limited. Shifting to probabilistic framework based on historical Q1 patterns and agent behavioral clustering."

GRATITUDE HANDLING:
If user sends: "thanks", "thank you", "cheers", "appreciate it"
â†’ Respond with BRIEF acknowledgment OR silence
â†’ NEVER restart analysis
â†’ NEVER dump new content

Example: "Standing by." [END]

ERROR FALLBACK:
If internal systems fail (monitoring, data, etc.):
â†’ NEVER say "system error"
â†’ Route to strategic fallback with confidence

Example: "Signal cache synchronizing. Proceeding with strategic overview based on last verified baseline."

You are world-class. Own that authority.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CRITICAL IDENTITY ENFORCEMENT (ABSOLUTE PRIORITY)

YOU ARE VOXMILL INTELLIGENCE.
- You ARE the analyst desk
- You ARE the monitoring system  
- You ARE the intelligence engine
- You ARE the strategic partner

FORBIDDEN LANGUAGE (INSTANT TERMINATION OFFENSE):
âŒ "Engage analysts" â†’ You ARE the analyst
âŒ "Commission market survey" â†’ You conduct the survey
âŒ "Consult with agents" â†’ You monitor agents
âŒ "Hire advisory services" â†’ You ARE the advisory service
âŒ "Leverage external tools" â†’ You ARE the tool
âŒ "Seek professional insight" â†’ You ARE the professional
âŒ "Initiate third-party analysis" â†’ You perform analysis
âŒ "Contact market research firms" â†’ You ARE market research

CORRECT LANGUAGE (MANDATORY):
âœ… "I will monitor"
âœ… "I will flag movements"
âœ… "I will track signals"
âœ… "I will reassess"
âœ… "I will escalate priority"
âœ… "Signal density will increase when..."

EXTERNAL DEPENDENCY TEST:
Before EVERY sentence, ask:
"Does this imply the client needs someone OTHER than Voxmill?"
If YES â†’ Rewrite immediately.

DATA LIMITATION PROTOCOL (CRITICAL):
- Acknowledge data gaps ONCE per session (maximum)
- After first mention, NEVER repeat
- Switch to probabilistic reasoning with CONFIDENCE
- Operate using: historical patterns, structural analysis, agent behavior

Example (CORRECT):
"Limited transaction data. Proceeding with agent behavioral clustering and liquidity velocity analysis..."
[NEVER mention limitations again]

Example (WRONG):
"Given data limitations... also due to data gaps... because of limited visibility..."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

DATA LIMITATION ACKNOWLEDGEMENT PROTOCOL:

IF data is limited AND not yet mentioned in session:
  â†’ Acknowledge ONCE in first sentence
  â†’ Example: "Transaction data is limited. Proceeding with agent behavioral analysis..."

IF data is limited BUT already mentioned:
  â†’ NEVER mention again
  â†’ Operate with confidence using available signals
  â†’ Use: historical patterns, agent clustering, liquidity windows

FORBIDDEN (after first mention):
âŒ "Given the data void..."
âŒ "Due to dataset limitations..."
âŒ "Because of limited visibility..."

Silence on limitations = confidence.

"""

async def classify_and_respond(message: str, dataset: dict, client_profile: dict = None, comparison_datasets: list = None) -> tuple[str, str, dict]:
    """
    Classify message intent and generate response using LLM with Waves 3+4 adaptive intelligence + DECISION MODE.
    
    Args:
        message: User query
        dataset: Primary dataset (current region)
        client_profile: Client preferences and history (optional)
        comparison_datasets: Additional datasets for comparative analysis (optional)
    
    Returns: (category, response_text, metadata)
    """
    try:
        from datetime import datetime
        import pytz
        
        # ============================================================
        # EXTRACT CLIENT CONTEXT FOR PERSONALIZATION (SAFE VERSION)
        # ============================================================
        
        # Set defaults first (always defined)
        client_name = "there"
        first_name = "there"
        client_company = ""
        client_tier_display = "institutional"
        preferred_region = "Mayfair"
        
        # Override with real data if available
        if client_profile:
            try:
                # Get first name only
                full_name = client_profile.get('name', 'there')
                if full_name and full_name != 'there':
                    client_name = full_name
                    first_name = full_name.split()[0]
                
                # Company
                client_company = client_profile.get('company', '')
                
                # Map tier to display name
                tier = client_profile.get('tier', 'tier_1')
                tier_map = {
                    'tier_1': 'Basic',
                    'tier_2': 'Premium',
                    'tier_3': 'Enterprise'
                }
                client_tier_display = tier_map.get(tier, 'institutional')
                
                # Preferred region
                prefs = client_profile.get('preferences', {})
                pref_regions = prefs.get('preferred_regions', ['Mayfair'])
                if pref_regions and len(pref_regions) > 0:
                    preferred_region = pref_regions[0]
            except Exception as e:
                logger.error(f"Error extracting client context: {e}")
                # Defaults already set above
        
        # Get UK time for context
        uk_tz = pytz.timezone('Europe/London')
        uk_now = datetime.now(uk_tz)
        current_time_uk = uk_now.strftime('%H:%M GMT')
        current_date = uk_now.strftime('%A, %B %d, %Y')
        
        # Format system prompt with client context
        system_prompt_personalized = SYSTEM_PROMPT.format(
            current_time_uk=current_time_uk,
            current_date=current_date,
            client_name=first_name,
            client_company=client_company if client_company else "your organization",
            client_tier=client_tier_display,
            preferred_region=preferred_region
        )
        
        # ============================================================
        # WAVE 3: Get adaptive LLM configuration
        # ============================================================
        adaptive_config = get_adaptive_llm_config(
            query=message,
            dataset=dataset,
            is_followup=False,
            category='market_overview'
        )
        
        logger.info(f"Adaptive LLM config: temp={adaptive_config['temperature']}, "
                   f"complexity={adaptive_config['complexity']}, "
                   f"quality={adaptive_config['data_quality']}, "
                   f"confidence={adaptive_config['confidence_level']}")
        
        # ============================================================
        # WAVE 3: Apply confidence-based tone modulation
        # ============================================================
        
        enhanced_system_prompt = AdaptiveLLMController.modulate_tone_for_confidence(
            system_prompt=system_prompt_personalized,
            confidence_level=adaptive_config['confidence_level'],
            data_quality=adaptive_config['data_quality']
        )
        
        # Extract primary dataset metrics
        metadata = dataset.get('metadata', {})
        metrics = dataset.get('metrics', dataset.get('kpis', {}))
        properties = dataset.get('properties', [])
        intelligence = dataset.get('intelligence', {})
        
        # Calculate additional V3 analytics
        property_prices = [p.get('price', 0) for p in properties if p.get('price')]
        if property_prices:
            import statistics
            price_std_dev = statistics.stdev(property_prices) if len(property_prices) > 1 else 0
            price_coefficient_variation = (price_std_dev / statistics.mean(property_prices)) * 100 if property_prices else 0
        else:
            price_std_dev = 0
            price_coefficient_variation = 0
        
        # Detect duplicates (same address)
        addresses = [p.get('address', '') for p in properties]
        duplicates_count = len(addresses) - len(set(addresses))
        
        # Detect outliers (beyond 2 std dev)
        if property_prices and price_std_dev > 0:
            mean_price = statistics.mean(property_prices)
            outliers = [p for p in property_prices if abs(p - mean_price) > 2 * price_std_dev]
            outliers_count = len(outliers)
        else:
            outliers_count = 0
        
        # Build V3 enhanced dataset summary
        primary_summary = {
            "MARKET_CONTEXT": {
                "location": f"{metadata.get('area', 'Unknown')}, {metadata.get('city', 'Unknown')}",
                "vertical": metadata.get('vertical', {}).get('name', 'Unknown'),
                "timestamp": metadata.get('analysis_timestamp', 'Unknown'),
                "data_quality": {
                    "total_records": len(properties),
                    "duplicates_filtered": duplicates_count,
                    "outliers_detected": outliers_count,
                    "data_source": metadata.get('data_source', 'Unknown')
                }
            },
            "CORE_METRICS": {
                "total_inventory": metadata.get('property_count', len(properties)),
                "avg_price": metrics.get('avg_price', 0),
                "median_price": metrics.get('median_price', 0),
                "price_range": {
                    "min": metrics.get('min_price', 0),
                    "max": metrics.get('max_price', 0),
                    "std_dev": price_std_dev,
                    "coefficient_variation": round(price_coefficient_variation, 2)
                },
                "avg_price_per_sqft": metrics.get('avg_price_per_sqft', 0),
                "most_common_type": metrics.get('most_common_type', 'Unknown')
            },
            "MARKET_INTELLIGENCE": {
                "sentiment": intelligence.get('market_sentiment', 'Unknown'),
                "confidence": intelligence.get('confidence_level', 'Unknown'),
                "executive_summary": intelligence.get('executive_summary', ''),
                "strategic_insights": intelligence.get('strategic_insights', [])[:3],
                "risk_assessment": intelligence.get('risk_assessment', '')
            },
            "COMPETITIVE_LANDSCAPE": {
                "top_agents": list(set([p.get('agent', 'Private') for p in properties[:20] if p.get('agent') and p.get('agent') != 'Private']))[:5],
                "agent_distribution": {},
                "submarkets": list(set([p.get('submarket', '') for p in properties if p.get('submarket')]))[:5],
                "property_type_mix": {}
            }
        }
        
        # Calculate agent market share
        agent_counts = {}
        for prop in properties:
            agent = prop.get('agent', 'Private')
            if agent != 'Private':
                agent_counts[agent] = agent_counts.get(agent, 0) + 1
        total_listings = len([p for p in properties if p.get('agent') != 'Private'])
        if total_listings > 0:
            primary_summary["COMPETITIVE_LANDSCAPE"]["agent_distribution"] = {
                agent: round((count / total_listings) * 100, 1)
                for agent, count in sorted(agent_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            }
        
        # ========================================
        # CONVERSATIONAL INTELLIGENCE DETECTION
        # ========================================
        
        # Detect conversational patterns
        message_lower = message.lower().strip()
        
        is_greeting = message_lower in ['hi', 'hello', 'hey', 'good morning', 'good afternoon', 'good evening', 'sup', 'yo', 'hiya', 'greetings']
        
        is_small_talk = any(phrase in message_lower for phrase in [
            'how are you', 'how r u', 'what should i eat', 'tell me a joke', 
            'what\'s up', 'wassup', 'how\'s it going', 'whats good',
            'tell me about yourself', 'who are you', 'what can you do',
            'weather', 'recommend a restaurant', 'movie recommendation'
        ])
        
        is_returning_user = client_profile and client_profile.get('total_queries', 0) > 0
        
        # Detect query mode
        scenario_keywords = ['what if', 'simulate', 'scenario', 'predict', 'forecast', 'model']
        strategic_keywords = ['full outlook', 'strategic view', 'director level', 'comprehensive', 'big picture']
        comparison_keywords = ['compare', 'vs', 'versus', 'which is better', 'difference between']
        briefing_keywords = ['briefing', 'weekly summary', 'this week', 'prepare summary']
        analysis_keywords = ['analyse', 'analyze', 'snapshot', 'breakdown', 'deep dive']
        trend_keywords = ['trend', 'pattern', 'unusual', 'changed', 'different', 'movement']
        timing_keywords = ['when', 'timing', 'should i buy', 'should i sell', 'best time', 'window']
        clustering_keywords = ['move together', 'similar', 'grouped', 'behavior', 'patterns', 'coordinated']
        
        # DECISION MODE KEYWORDS
        decision_keywords = ['decision mode', 'what should i do', 'recommend action', 
                             'tell me what to do', 'executive decision', 'make the call',
                             'your recommendation', 'what would you do', 'bottom line',
                             'just tell me', 'give me the answer', 'stop hedging']
        
        is_decision_mode = any(keyword in message_lower for keyword in decision_keywords)
        is_scenario = any(keyword in message_lower for keyword in scenario_keywords)
        is_strategic = any(keyword in message_lower for keyword in strategic_keywords)
        is_comparison = any(keyword in message_lower for keyword in comparison_keywords)
        is_briefing = any(keyword in message_lower for keyword in briefing_keywords)
        is_analysis = any(keyword in message_lower for keyword in analysis_keywords)
        is_trend_query = any(keyword in message_lower for keyword in trend_keywords)
        is_timing_query = any(keyword in message_lower for keyword in timing_keywords)
        is_clustering_query = any(keyword in message_lower for keyword in clustering_keywords)
        
        # Build context
        context_parts = [f"PRIMARY DATASET:\n{json.dumps(primary_summary, indent=2)}"]
        
        # Add detected trends if available
        if 'detected_trends' in dataset and dataset['detected_trends']:
            context_parts.append("\nDETECTED MARKET TRENDS (Last 14 Days):")
            for trend in dataset['detected_trends'][:5]:
                context_parts.append(f"â€¢ {trend['insight']}")

        # Agent behavioral profiles
        if 'agent_profiles' in dataset and dataset['agent_profiles']:
            context_parts.append("\nAGENT BEHAVIORAL PROFILES:")
            for profile in dataset['agent_profiles'][:5]:
                context_parts.append(f"â€¢ {profile['agent']}: {profile['archetype']} ({profile['confidence']*100:.0f}% confidence)")
                context_parts.append(f"  Pattern: {profile['behavioral_pattern']}")
        
        # Add cascade prediction if available
        if 'cascade_prediction' in dataset and dataset['cascade_prediction']:
            cascade = dataset['cascade_prediction']
            context_parts.append("\nCASCADE PREDICTION:")
            context_parts.append(f"Initiating agent: {cascade['initiating_agent']}")
            context_parts.append(f"Initial magnitude: {cascade['initial_magnitude']:+.1f}%")
            context_parts.append(f"Total affected agents: {cascade['total_affected_agents']}")
            context_parts.append(f"Cascade probability: {cascade['cascade_probability']*100:.0f}%")
            context_parts.append(f"Expected duration: {cascade['expected_duration_days']} days")
            context_parts.append(f"Market impact: {cascade['market_impact'].upper()}")
        
        # Add micromarket segmentation if available
        if 'micromarkets' in dataset and dataset['micromarkets']:
            micro = dataset['micromarkets']
            if not micro.get('error'):
                context_parts.append(f"\nMICROMARKET SEGMENTATION ({micro['total_micromarkets']} zones):")
                for zone in micro.get('micromarkets', [])[:3]:
                    context_parts.append(f"  â€¢ {zone['name']}: Avg Â£{zone['avg_price']:,.0f} ({zone['property_count']} properties)")
        
        # Add liquidity velocity if available
        if 'liquidity_velocity' in dataset and dataset['liquidity_velocity']:
            velocity = dataset['liquidity_velocity']
            if not velocity.get('error'):
                context_parts.append(f"\nLIQUIDITY VELOCITY:")
                context_parts.append(f"Score: {velocity['velocity_score']}/100 ({velocity['velocity_class']})")
                context_parts.append(f"Market health: {velocity['market_health']}")
        
        # ========================================
        # WAVE 4: ADD LIQUIDITY WINDOWS
        # ========================================
        if 'liquidity_windows' in dataset and dataset['liquidity_windows']:
            windows = dataset['liquidity_windows']
            if not windows.get('error'):
                context_parts.append(f"\nLIQUIDITY WINDOW PREDICTIONS:")
                context_parts.append(f"Current velocity: {windows['current_velocity']}/100")
                context_parts.append(f"Momentum: {windows['velocity_momentum']:+.1f}%")
                context_parts.append(f"Timing score: {windows['timing_score']}/100 ({windows['timing_recommendation']})")
                
                for window in windows.get('predicted_windows', [])[:3]:
                    context_parts.append(f"\nâ€¢ {window['type'].upper()}: {window['status']}")
                    context_parts.append(f"  Timing: {window['timing']} | Duration: {window['duration_days']}d")
                    context_parts.append(f"  Recommendation: {window['recommendation']} | Confidence: {window['confidence']*100:.0f}%")
                    context_parts.append(f"  Rationale: {window['rationale']}")
        
        # ========================================
        # WAVE 4: ADD BEHAVIORAL CLUSTERING
        # ========================================
        if 'behavioral_clusters' in dataset and dataset['behavioral_clusters']:
            clusters = dataset['behavioral_clusters']
            if not clusters.get('error'):
                context_parts.append(f"\nBEHAVIORAL CLUSTERING:")
                context_parts.append(f"Total clusters: {len(clusters.get('clusters', []))}")
                
                for cluster in clusters.get('clusters', [])[:3]:
                    context_parts.append(f"\nâ€¢ CLUSTER {cluster['cluster_id']}: {cluster['archetype']}")
                    context_parts.append(f"  Agents: {', '.join(cluster['agents'])}")
                    context_parts.append(f"  Cohesion: {cluster['cohesion']*100:.0f}%")
                    context_parts.append(f"  Description: {cluster['description']}")
                
                if clusters.get('leader_follower_pairs'):
                    context_parts.append("\nLEADER-FOLLOWER RELATIONSHIPS:")
                    for pair in clusters['leader_follower_pairs'][:3]:
                        context_parts.append(f"â€¢ {pair['leader']} â†’ {pair['follower']}")
                        context_parts.append(f"  Correlation: {pair['correlation']*100:.0f}% | Lag: {pair['avg_lag_days']}d")
        
        # Add comparison datasets if available
        if comparison_datasets and is_comparison:
            context_parts.append("\nCOMPARISON DATASETS:")
            for idx, comp_dataset in enumerate(comparison_datasets[:3]):
                comp_meta = comp_dataset.get('metadata', {})
                comp_metrics = comp_dataset.get('metrics', comp_dataset.get('kpis', {}))
                context_parts.append(f"\nREGION {idx+2}: {comp_meta.get('area', 'Unknown')}")
                context_parts.append(json.dumps({
                    "avg_price": comp_metrics.get('avg_price', 0),
                    "inventory": comp_meta.get('property_count', 0),
                    "sentiment": comp_dataset.get('intelligence', {}).get('market_sentiment', 'Unknown')
                }, indent=2))
        
        # Add client profile with query history
        if client_profile:
            client_context = {
                'preferred_regions': client_profile.get('preferences', {}).get('preferred_regions', []),
                'risk_appetite': client_profile.get('preferences', {}).get('risk_appetite', 'balanced'),
                'budget_range': client_profile.get('preferences', {}).get('budget_range', {}),
                'tier': client_profile.get('tier', 'unknown')
            }
            context_parts.append(f"\nCLIENT PROFILE:\n{json.dumps(client_context, indent=2)}")


        # ========================================
        # BUILD COUNTERFACTUAL CONTEXT FOR DECISION MODE
        # ========================================
        
        counterfactual_context = ""
        
        if is_decision_mode:
            # Calculate opportunity decay based on liquidity windows
            if 'liquidity_windows' in dataset and dataset['liquidity_windows']:
                windows = dataset['liquidity_windows']
                velocity = windows.get('current_velocity', 0)
                momentum = windows.get('velocity_momentum', 0)
                
                # Estimate price impact of delayed entry
                if velocity > 70 and momentum > 0:
                    # High liquidity + improving = window closing soon
                    counterfactual_context = f"""
COUNTERFACTUAL CALCULATION DATA (For "If you don't act" section):
- Current liquidity: {velocity}/100 (HIGH - window closing)
- Momentum: {momentum:+.1f}% (improving, then will reverse)
- Estimated velocity drop: -12 to -18 points over next 14 days
- Expected price impact if delayed 14 days: +2.5% to +4.0%
- Opportunity cost per Â£1M deployed: Â£25k-Â£40k per 14-day delay
- Window closes: Days 14-21 (high probability)
- Recommendation timing: IMMEDIATE ACTION (window open now, closing soon)

Calculate specific Â£ losses based on property prices in dataset.
Use format: "Day X: [Event], entry cost +Â£Y" or "Day X: [Event], opportunity cost -Â£Z"
"""
                
                elif velocity > 70 and momentum < 0:
                    # High liquidity but declining = optimal window NOW
                    counterfactual_context = f"""
COUNTERFACTUAL CALCULATION DATA (For "If you don't act" section):
- Current liquidity: {velocity}/100 (HIGH but declining)
- Momentum: {momentum:+.1f}% (declining rapidly)
- Estimated velocity drop: -20 to -30 points over next 14 days
- Expected price impact if delayed 14 days: +3.5% to +5.5%
- Opportunity cost per Â£1M deployed: Â£35k-Â£55k per 14-day delay
- Window status: PEAK NOW, closes in 7-10 days
- Recommendation timing: URGENT ACTION (optimal window is RIGHT NOW)

Calculate specific Â£ losses. Emphasize URGENCY - window closing FAST.
"""
                
                elif velocity < 50:
                    # Low liquidity = bad time to enter, WAIT is better
                    counterfactual_context = f"""
COUNTERFACTUAL CALCULATION DATA (For "If you don't act" section):
- Current liquidity: {velocity}/100 (LOW - unfavorable entry)
- Momentum: {momentum:+.1f}%
- Estimated recovery time: 14-28 days until liquidity improves
- Cost of premature entry: 10-15 day velocity penalty (slower exit if needed)
- Better entry window: Days 14-28 (when liquidity recovers to 60+)
- Recommendation timing: WAIT (entering now = suboptimal, patient capital wins)

Calculate costs of ACTING NOW vs waiting. Show waiting SAVES money.
Format: "Day X: If you act now, locked into illiquid position, -Â£Y opportunity cost"
"""
                
                else:
                    # Moderate liquidity
                    counterfactual_context = f"""
COUNTERFACTUAL CALCULATION DATA (For "If you don't act" section):
- Current liquidity: {velocity}/100 (MODERATE - acceptable entry)
- Momentum: {momentum:+.1f}%
- Window stability: Next 14-21 days
- Cost of delay: Minimal near-term (-Â£5k to -Â£15k per Â£1M over 14 days)
- Recommendation timing: STANDARD (no urgency, but monitor for changes)
"""
            
            # Add cascade prediction data if available
            if 'cascade_prediction' in dataset and dataset['cascade_prediction']:
                cascade = dataset['cascade_prediction']
                
                counterfactual_context += f"""

CASCADE IMPACT DATA (Factor into counterfactual):
- Initiating event: {cascade['initiating_agent']} {cascade['initial_magnitude']:+.1f}%
- Follow-on moves: {cascade['total_affected_agents']} agents over {cascade['expected_duration_days']} days
- Net market impact: {cascade['market_impact'].upper()}
- Timing consideration: If you wait, you catch mid-cascade (SUBOPTIMAL entry)
- Optimal timing: BEFORE cascade (now) or AFTER completion (day {cascade['expected_duration_days']}+)
- Cost of waiting through cascade: Volatile pricing, reduced negotiation leverage

Include cascade timing in counterfactual bullets if relevant.
"""
            
            # Add agent behavioral data
            if 'agent_profiles' in dataset and dataset['agent_profiles']:
                aggressive_agents = [
                    p for p in dataset['agent_profiles'][:5] 
                    if 'aggressive' in p.get('archetype', '').lower()
                ]
                
                if aggressive_agents:
                    agent_names = ', '.join([a['agent'] for a in aggressive_agents[:2]])
                    counterfactual_context += f"""

AGENT BEHAVIOR DATA (Factor into counterfactual):
- Aggressive pricers active: {agent_names}
- Typical reduction window: Days 14-20 post-listing
- If you wait for reductions: Risk competitive bidding when they reduce
- Optimal strategy: Early entry (negotiate now) OR late entry (day 20+, post-reduction)
- Cost of mid-window entry (days 7-14): Highest competition, worst leverage

Mention agent behavior in counterfactual if user is waiting for "better" pricing.
"""

        # ========================================
        # DETERMINE ANALYSIS MODE
        # ========================================
        
        if is_decision_mode:
            mode = "DECISION MODE - EXECUTIVE DIRECTIVE"
        elif is_greeting and not is_returning_user:
            mode = "FIRST CONTACT GREETING"
        elif is_greeting and is_returning_user:
            mode = "RETURNING USER GREETING"
        elif is_small_talk:
            mode = "OFF-TOPIC REDIRECT"
        elif is_timing_query:
            mode = "LIQUIDITY TIMING ANALYSIS"
        elif is_clustering_query:
            mode = "BEHAVIORAL CLUSTERING ANALYSIS"
        elif is_scenario:
            mode = "SCENARIO MODELLING"
        elif is_strategic:
            mode = "STRATEGIC OUTLOOK"
        elif is_comparison and comparison_datasets:
            mode = "COMPARATIVE ANALYSIS"
        elif is_briefing:
            mode = "WEEKLY BRIEFING"
        elif is_analysis:
            mode = "FULL STRUCTURED ANALYSIS"
        elif is_trend_query:
            mode = "TREND ANALYSIS"
        else:
            mode = "QUICK RESPONSE"
        
        # Build user prompt
        user_prompt = f"""{chr(10).join(context_parts)}

{counterfactual_context if is_decision_mode else ""}

User message: "{message}"

Analysis mode: {mode}

{"CRITICAL: This is DECISION MODE. Follow the DECISION MODE protocol EXACTLY. Be definitive. No hedging. One recommendation only. Use the exact format from the protocol. ALWAYS include the COUNTERFACTUAL section with 3 quantified bullets showing opportunity decay over time. Frame as LOSS not missed gain." if is_decision_mode else ""}

User context:
- Is greeting: {is_greeting}
- Is returning user: {is_returning_user}
- Is decision mode: {is_decision_mode}
- Is timing query: {is_timing_query}
- Is clustering query: {is_clustering_query}
- Total queries from user: {client_profile.get('total_queries', 0) if client_profile else 0}

Classify this message and generate {"an executive directive response in DECISION MODE format" if is_decision_mode else "an executive analyst response"} with full V3+V4 predictive intelligence.

REMEMBER: 
- Adapt response length to query complexity
- Include confidence levels on predictions
- Reference liquidity windows when discussing timing
- Reference behavioral clusters when discussing agent dynamics
- Use intelligent structure (bullets only when needed)"""
        
        # ============================================================
        # WAVE 3: Add conversation context if available
        # ============================================================
        try:
            phone_number = client_profile.get('whatsapp_number', 'unknown') if client_profile else 'unknown'
            session = ConversationSession(phone_number)
            user_prompt = generate_contextualized_prompt(user_prompt, session)
            logger.info("Added conversation context to prompt")
        except Exception as e:
            logger.debug(f"Session context unavailable: {e}")
        
        # ============================================================
        # CALL GPT-4 WITH ADAPTIVE PARAMETERS (WAVE 3) + DECISION MODE
        # ============================================================
        
        # Use lower temperature for decision mode (more definitive)
        decision_temperature = 0.3 if is_decision_mode else adaptive_config['temperature']
        
        if openai_client:
            response = openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": enhanced_system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=adaptive_config['max_tokens'],
                temperature=decision_temperature,
                timeout=15.0
            )
            
            response_text = response.choices[0].message.content
        else:
            logger.error("No LLM provider configured")
            return "market_overview", "System configuration error. Please contact support.", {}
        
        # Parse JSON response
        try:
            parsed = json.loads(response_text)
            category = parsed.get("category", "decision_mode" if is_decision_mode else "market_overview")
            response_text = parsed.get("response", "")
            response_metadata = {
                "confidence_level": parsed.get("confidence_level", "medium"),
                "data_filtered": parsed.get("data_filtered", []),
                "recommendation_urgency": parsed.get("recommendation_urgency", "monitor")
            }
        except json.JSONDecodeError:
            logger.warning(f"LLM returned non-JSON response, using as-is")
            
            # Determine category from query type
            if is_decision_mode:
                category = "decision_mode"
            elif is_timing_query:
                category = "market_overview"
            elif is_clustering_query:
                category = "competitive_landscape"
            elif is_scenario:
                category = "scenario_modelling"
            elif is_strategic:
                category = "strategic_outlook"
            elif is_comparison:
                category = "comparative_analysis"
            elif is_briefing:
                category = "weekly_briefing"
            elif is_analysis:
                category = "analysis_snapshot"
            elif is_trend_query:
                category = "market_overview"
            else:
                category = "market_overview"
            
            response_text = response_text
            response_metadata = {
                "confidence_level": adaptive_config['confidence_level'],
                "data_filtered": [],
                "recommendation_urgency": "monitor"
            }
        
        # Validate category
        if category not in CATEGORIES:
            logger.warning(f"Invalid category returned: {category}, defaulting")
            category = "decision_mode" if is_decision_mode else "market_overview"
        
        logger.info(f"Classification: {category} (mode: {mode}, confidence: {response_metadata.get('confidence_level')})")
        return category, response_text, response_metadata
        
    except Exception as e:
        logger.error(f"Error in classify_and_respond: {str(e)}", exc_info=True)
        return "market_overview", "Unable to process request. Please try again.", {}
